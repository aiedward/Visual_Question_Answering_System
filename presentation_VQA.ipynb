{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Question and Answering using MS COCO dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "- Introduction\n",
    "- Data\n",
    "- Methodology\n",
    "- Evaluation\n",
    "- Testing with new images\n",
    "- Moving Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VISUALQA.ORG\n",
    "- Ongoing competition from Virginia Tech and Microsoft \n",
    "- successor to the MS COCO image captioning contest\n",
    "- Has over 250k pictures along with 750k questions and answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/iamaaditya/VQA_Demo\n",
    "<p>\n",
    "https://github.com/avisingh599/visual-qa\n",
    "<p>\n",
    "https://github.com/VT-vision-lab/VQA_LSTM_CNN\n",
    "<p>\n",
    "http://karpathy.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/presentation/sample.png\" alt=\"\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions to Ask\n",
    "- Can the model understand the question?\n",
    "- Can the model extract subject of the question and match it to a particular image?\n",
    "- Can the model give reasonable best guess if match not found?\n",
    "- Can the model localize segments of the image to identify objects?\n",
    "- Can the model detect multiple objects in an image?\n",
    "- How can we get a list of possible answers and their corresponding probabilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platform and Libraries\n",
    "- Keras with Theano backend\n",
    "- spacy word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/presentation/microsoft_coco.png\" alt=\"\" style=\"width: 300px;\"/>\n",
    "<img src=\"data/presentation/data.png\" alt=\"\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CNN model used VGG16(16-layer structure from Visual Geometry Group mean for image classification task) and corresponding weights to train on the images.\n",
    "    - We wanted to do our own image feature extraction but it would've taken us two weeks in computation time alone.\n",
    "- Word embeddings - GloVe(Global Vectors for Word Representation) on spacy's platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/presentation/mikediagram.png\" alt=\"\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras' Built-in Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/presentation/model.png\" alt=\"\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/presentation/vgg.png\" alt=\"\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **LSTM with RMSprop optimizer**(lr= 0.001, rho = 0.9, epsilon = 1e-06) (as per Karpathy's blog)\n",
    "    - 24.38 minutes training per epoch\n",
    "- **GRU with RMSprop optimizer**(lr = 0.001, rho = 0.9, epsilon = 1e-06)\n",
    "    - 21.37 minutes training per epoch\n",
    "- **LSTM with SGD optimizer**(lr=0.01, momentum=0.9, decay=0.0, nesterov=True)\n",
    "    - 18.75 minutes training per epoch\n",
    "- We also tested GRU with Adam, GRU with SGD, and LSTM with Adam but they were giving us exceptionally high training loss from the first epoch so we stopped them early."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test models to epoch 10\n",
    "- Because we were getting horrendous evaluation results, our criteria for success in measuring accuracy was, if the correct answer was in the top 10 and top 5 of our answer distribution, it was a success. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "three = pd.read_pickle(\"eva.pkl\")\n",
    "ten = pd.read_pickle(\"five.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM+RMSpop</td>\n",
       "      <td>0.542857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GRU+RMSprop</td>\n",
       "      <td>0.495238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM+SGD</td>\n",
       "      <td>0.076190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model       val\n",
       "0  LSTM+RMSpop  0.542857\n",
       "1  GRU+RMSprop  0.495238\n",
       "2     LSTM+SGD  0.076190"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM+RMSprop</td>\n",
       "      <td>0.219048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GRU+RMSprop</td>\n",
       "      <td>0.495238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM+SGD</td>\n",
       "      <td>0.057143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model       val\n",
       "0  LSTM+RMSprop  0.219048\n",
       "1   GRU+RMSprop  0.495238\n",
       "2      LSTM+SGD  0.057143"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM + RMSprop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/presentation/graphs.png\" alt=\"\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRU + RMSprop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/presentation/graphs2.png\" alt=\"\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM + SGD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/presentation/graphs3.png\" alt=\"\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with new images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/presentation/bicycle.jpg\" alt=\"\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question about the image:what is in the picture?\n",
      "09.96 %  bicycle\n",
      "04.78 %  bike\n",
      "03.91 %  motorcycle\n",
      "02.21 %  brick\n",
      "01.79 %  dog\n",
      "01.62 %  umbrella\n",
      "01.62 %  carriage\n",
      "01.52 %  motorcycles\n",
      "01.35 %  cat\n",
      "01.33 %  luggage\n",
      "Ask a question about the image:how many wheels does the bike have?\n",
      "51.32 %  2\n",
      "009.0 %  5\n",
      "05.53 %  8\n",
      "03.95 %  3\n",
      "003.8 %  0\n",
      "003.1 %  1\n",
      "02.85 %  10\n",
      "002.6 %  20\n",
      "002.4 %  7\n",
      "02.23 %  9\n",
      "Ask a question about the image:what direction is the bike facing?\n",
      "51.61 %  left\n",
      "14.61 %  right\n",
      "02.19 %  outside\n",
      "01.12 %  man\n",
      "00.91 %  no\n",
      "000.7 %  yes\n",
      "00.49 %  winter\n",
      "00.38 %  top\n",
      "00.33 %  no one\n",
      "00.32 %  fall\n",
      "Ask a question about the image:what color is the bike?\n",
      "25.66 %  green\n",
      "17.14 %  blue\n",
      "013.5 %  black\n",
      "12.07 %  yellow\n",
      "10.57 %  red\n",
      "05.81 %  white\n",
      "03.55 %  orange\n",
      "01.53 %  gray\n",
      "01.39 %  purple\n",
      "01.21 %  brown\n",
      "Ask a question about the image:how many cars in the background?\n",
      "51.18 %  2\n",
      "09.04 %  5\n",
      "05.56 %  8\n",
      "03.95 %  3\n",
      "03.82 %  0\n",
      "03.09 %  1\n",
      "02.85 %  10\n",
      "02.62 %  20\n",
      "002.4 %  7\n",
      "02.25 %  9\n"
     ]
    }
   ],
   "source": [
    "from prediction import predict\n",
    "\n",
    "model = 'models/lstm_1_num_hidden_units_lstm_512_num_hidden_units_mlp_1024_num_hidden_layers_mlp_3_num_hidden_layers_lstm_1.json'\n",
    "weights = 'models/lstm_1_num_hidden_units_lstm_512_num_hidden_units_mlp_1024_num_hidden_layers_mlp_3_num_hidden_layers_lstm_1_epoch_099.hdf5'\n",
    "image_directory = 'data/mike_photos/bicycle.jpg'\n",
    "predict(model, weights, image_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things we have noticed after testing with multiple images\n",
    "- Counting objects is hardly ever accurate\n",
    "- Yes/No questions are also usually almost always the opposite\n",
    "- Determing color of objects is not easy\n",
    "- Best result is just \"what is in the picture?\"\n",
    "- There are limitations to an answer space of 1000 words\n",
    "    - we input a picture of a rabbit\n",
    "    - got back bear, cat, duck, seagull...no rabbit or bunny in the answer space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Challenges / Moving Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TRAINING TIME - We read other participants in the contest were able to get training times of 5 minutes for 20 epochs with a powerful GPU. Must invest in an nVidia graphics card or AWS.\n",
    "- Once graphics card is purchased, we should try changing the parameters of the optimizer(we used default parameters because of time constraints)\n",
    "- Evaluate VGG features - we don't know the specifics of the model and whether something about it is causing our discrepant results\n",
    "- Train our own image features\n",
    "- Try Multiple Layer LSTMs and compare it with our model\n",
    "    -Bidirectional LSTMs?\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
